{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Problem Statement**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Business Context**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "An automobile dealership in Los Vegas specializes in selling luxury and non-luxury vehicles. They cater to diverse customer preferences with varying vehicle specifications, such as mileage, engine capacity, and seating capacity. However, the dealership faces significant challenges in maintaining consistency and efficiency across its pricing strategy due to reliance on manual processes and disconnected systems. Pricing evaluations are prone to errors, updates are delayed, and scaling operations are difficult as demand grows. These inefficiencies impact revenue and customer trust. Recognizing the need for a reliable and scalable solution, the dealership is seeking to implement a unified system that ensures seamless integration of data-driven pricing decisions, adaptability to changing market conditions, and operational efficiency."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Objective**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dealership has hired you as an MLOps Engineer to design and implement an MLOps pipeline that automates the pricing workflow. This pipeline will encompass data cleaning, preprocessing, transformation, model building, training, evaluation, and registration with CI/CD capabilities to ensure continuous integration and delivery. Your role is to overcome challenges such as integrating disparate data sources, maintaining consistent model performance, and enabling scalable, automated updates to meet evolving business needs. The expected outcomes are a robust, automated system that improves pricing accuracy, operational efficiency, and scalability, driving increased profitability and customer satisfaction."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Data Description**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset contains attributes of used cars sold in various locations. These attributes serve as key data points for CarOnSell's pricing model. The detailed attributes are:\n",
        "\n",
        "- **Segment:** Describes the category of the vehicle, indicating whether it is a luxury or non-luxury segment.\n",
        "\n",
        "- **Kilometers_Driven:** The total number of kilometers the vehicle has been driven.\n",
        "\n",
        "- **Mileage:** The fuel efficiency of the vehicle, measured in kilometers per liter (km/l).\n",
        "\n",
        "- **Engine:** The engine capacity of the vehicle, measured in cubic centimeters (cc). \n",
        "\n",
        "- **Power:** The power of the vehicle's engine, measured in brake horsepower (BHP). \n",
        "\n",
        "- **Seats:** The number of seats in the vehicle, can influence the vehicle's classification, usage, and pricing based on customer needs.\n",
        "\n",
        "- **Price:** The price of the vehicle, listed in lakhs (units of 100,000), represents the cost to the consumer for purchasing the vehicle."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GItHub Repo Link**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "https://github.com/rakshit2711/cars"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. AzureML Environment Setup and Data Preparation**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.1 Connect to Azure Machine Learning Workspace**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing dependencies"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install azure-ai-ml azure-identity"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting azure-ai-ml\n  Downloading azure_ai_ml-1.29.0-py3-none-any.whl (13.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hRequirement already satisfied: azure-identity in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (1.21.0)\nRequirement already satisfied: azure-core>=1.23.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (1.33.0)\nRequirement already satisfied: pyjwt<3.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (2.4.0)\nCollecting azure-monitor-opentelemetry\n  Downloading azure_monitor_opentelemetry-1.8.1-py3-none-any.whl (27 kB)\nRequirement already satisfied: azure-storage-blob>=12.10.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (12.25.1)\nCollecting strictyaml<2.0.0\n  Downloading strictyaml-1.7.3-py3-none-any.whl (123 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.9/123.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: isodate<1.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (0.7.2)\nRequirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (4.24.0)\nCollecting azure-storage-file-datalake>=12.2.0\n  Downloading azure_storage_file_datalake-12.21.0-py3-none-any.whl (264 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.1/264.1 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm<5.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (4.67.1)\nRequirement already satisfied: azure-mgmt-core>=1.3.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (1.5.0)\nRequirement already satisfied: colorama<1.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (0.4.6)\nCollecting pydash<9.0.0,>=6.0.0\n  Downloading pydash-8.0.5-py3-none-any.whl (102 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.1/102.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: azure-common>=1.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (1.1.28)\nRequirement already satisfied: typing-extensions<5.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (4.14.1)\nCollecting marshmallow<4.0.0,>=3.5\n  Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyyaml<7.0.0,>=5.1.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-ai-ml) (6.0.2)\nCollecting azure-storage-file-share\n  Downloading azure_storage_file_share-12.22.0-py3-none-any.whl (291 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.3/291.3 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: msal-extensions>=1.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-identity) (1.2.0)\nRequirement already satisfied: msal>=1.30.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-identity) (1.33.0b1)\nRequirement already satisfied: cryptography>=2.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-identity) (38.0.4)\nRequirement already satisfied: requests>=2.21.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-core>=1.23.0->azure-ai-ml) (2.32.4)\nRequirement already satisfied: six>=1.11.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-core>=1.23.0->azure-ai-ml) (1.17.0)\nCollecting azure-storage-blob>=12.10.0\n  Downloading azure_storage_blob-12.26.0-py3-none-any.whl (412 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.9/412.9 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: cffi>=1.12 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from cryptography>=2.5->azure-identity) (1.17.1)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (2025.4.1)\nRequirement already satisfied: referencing>=0.28.4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (0.36.2)\nRequirement already satisfied: rpds-py>=0.7.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (0.24.0)\nRequirement already satisfied: attrs>=22.2.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->azure-ai-ml) (25.3.0)\nRequirement already satisfied: packaging>=17.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from marshmallow<4.0.0,>=3.5->azure-ai-ml) (25.0)\nRequirement already satisfied: portalocker<3,>=1.4 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from msal-extensions>=1.2.0->azure-identity) (2.10.1)\nRequirement already satisfied: python-dateutil>=2.6.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from strictyaml<2.0.0->azure-ai-ml) (2.9.0.post0)\nCollecting opentelemetry-instrumentation-psycopg2~=0.57b0\n  Downloading opentelemetry_instrumentation_psycopg2-0.58b0-py3-none-any.whl (10 kB)\nCollecting opentelemetry-sdk~=1.36\n  Downloading opentelemetry_sdk-1.37.0-py3-none-any.whl (131 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.9/131.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting opentelemetry-instrumentation-fastapi~=0.57b0\n  Downloading opentelemetry_instrumentation_fastapi-0.58b0-py3-none-any.whl (13 kB)\nCollecting opentelemetry-instrumentation-urllib3~=0.57b0\n  Downloading opentelemetry_instrumentation_urllib3-0.58b0-py3-none-any.whl (13 kB)\nCollecting opentelemetry-resource-detector-azure~=0.1.5\n  Downloading opentelemetry_resource_detector_azure-0.1.5-py3-none-any.whl (14 kB)\nCollecting opentelemetry-instrumentation-flask~=0.57b0\n  Downloading opentelemetry_instrumentation_flask-0.58b0-py3-none-any.whl (14 kB)\nCollecting azure-core-tracing-opentelemetry~=1.0.0b11\n  Downloading azure_core_tracing_opentelemetry-1.0.0b12-py3-none-any.whl (11 kB)\nCollecting azure-monitor-opentelemetry-exporter~=1.0.0b41\n  Downloading azure_monitor_opentelemetry_exporter-1.0.0b42-py2.py3-none-any.whl (183 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.3/183.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting opentelemetry-instrumentation-urllib~=0.57b0\n  Downloading opentelemetry_instrumentation_urllib-0.58b0-py3-none-any.whl (12 kB)\nCollecting opentelemetry-instrumentation-django~=0.57b0\n  Downloading opentelemetry_instrumentation_django-0.58b0-py3-none-any.whl (19 kB)\nCollecting opentelemetry-instrumentation-requests~=0.57b0\n  Downloading opentelemetry_instrumentation_requests-0.58b0-py3-none-any.whl (12 kB)\nRequirement already satisfied: opentelemetry-api>=1.12.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-core-tracing-opentelemetry~=1.0.0b11->azure-monitor-opentelemetry->azure-ai-ml) (1.35.0)\nCollecting fixedint==0.1.6\n  Downloading fixedint-0.1.6-py3-none-any.whl (12 kB)\nCollecting psutil<8,>=5.9\n  Downloading psutil-7.1.0-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.2/291.2 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: msrest>=0.6.10 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from azure-monitor-opentelemetry-exporter~=1.0.0b41->azure-monitor-opentelemetry->azure-ai-ml) (0.7.1)\nRequirement already satisfied: pycparser in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity) (2.22)\nCollecting opentelemetry-util-http==0.58b0\n  Downloading opentelemetry_util_http-0.58b0-py3-none-any.whl (7.7 kB)\nCollecting opentelemetry-instrumentation==0.58b0\n  Downloading opentelemetry_instrumentation-0.58b0-py3-none-any.whl (33 kB)\nCollecting opentelemetry-instrumentation-wsgi==0.58b0\n  Downloading opentelemetry_instrumentation_wsgi-0.58b0-py3-none-any.whl (14 kB)\nCollecting opentelemetry-semantic-conventions==0.58b0\n  Downloading opentelemetry_semantic_conventions-0.58b0-py3-none-any.whl (207 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.0/208.0 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.58b0->opentelemetry-instrumentation-django~=0.57b0->azure-monitor-opentelemetry->azure-ai-ml) (1.14.1)\nCollecting opentelemetry-api>=1.12.0\n  Downloading opentelemetry_api-1.37.0-py3-none-any.whl (65 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from opentelemetry-api>=1.12.0->azure-core-tracing-opentelemetry~=1.0.0b11->azure-monitor-opentelemetry->azure-ai-ml) (8.2.0)\nCollecting opentelemetry-instrumentation-asgi==0.58b0\n  Downloading opentelemetry_instrumentation_asgi-0.58b0-py3-none-any.whl (16 kB)\nCollecting asgiref~=3.0\n  Downloading asgiref-3.10.0-py3-none-any.whl (24 kB)\nCollecting opentelemetry-instrumentation-dbapi==0.58b0\n  Downloading opentelemetry_instrumentation_dbapi-0.58b0-py3-none-any.whl (12 kB)\nRequirement already satisfied: charset_normalizer<4,>=2 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-ai-ml) (3.4.2)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-ai-ml) (2025.7.9)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-ai-ml) (2.5.0)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-ai-ml) (3.10)\nRequirement already satisfied: requests-oauthlib>=0.5.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from msrest>=0.6.10->azure-monitor-opentelemetry-exporter~=1.0.0b41->azure-monitor-opentelemetry->azure-ai-ml) (2.0.0)\nRequirement already satisfied: zipp>=0.5 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.12.0->azure-core-tracing-opentelemetry~=1.0.0b11->azure-monitor-opentelemetry->azure-ai-ml) (3.19.2)\nRequirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.10->azure-monitor-opentelemetry-exporter~=1.0.0b41->azure-monitor-opentelemetry->azure-ai-ml) (3.2.2)\nInstalling collected packages: fixedint, pydash, psutil, opentelemetry-util-http, marshmallow, asgiref, strictyaml, opentelemetry-api, opentelemetry-semantic-conventions, azure-storage-file-share, azure-storage-blob, azure-core-tracing-opentelemetry, opentelemetry-sdk, opentelemetry-instrumentation, azure-storage-file-datalake, opentelemetry-resource-detector-azure, opentelemetry-instrumentation-wsgi, opentelemetry-instrumentation-urllib3, opentelemetry-instrumentation-urllib, opentelemetry-instrumentation-requests, opentelemetry-instrumentation-dbapi, opentelemetry-instrumentation-asgi, opentelemetry-instrumentation-psycopg2, opentelemetry-instrumentation-flask, opentelemetry-instrumentation-fastapi, opentelemetry-instrumentation-django, azure-monitor-opentelemetry-exporter, azure-monitor-opentelemetry, azure-ai-ml\n  Attempting uninstall: psutil\n    Found existing installation: psutil 5.2.2\n    Uninstalling psutil-5.2.2:\n      Successfully uninstalled psutil-5.2.2\n  Attempting uninstall: opentelemetry-api\n    Found existing installation: opentelemetry-api 1.35.0\n    Uninstalling opentelemetry-api-1.35.0:\n      Successfully uninstalled opentelemetry-api-1.35.0\n  Attempting uninstall: opentelemetry-semantic-conventions\n    Found existing installation: opentelemetry-semantic-conventions 0.56b0\n    Uninstalling opentelemetry-semantic-conventions-0.56b0:\n      Successfully uninstalled opentelemetry-semantic-conventions-0.56b0\n  Attempting uninstall: azure-storage-blob\n    Found existing installation: azure-storage-blob 12.25.1\n    Uninstalling azure-storage-blob-12.25.1:\n      Successfully uninstalled azure-storage-blob-12.25.1\n  Attempting uninstall: opentelemetry-sdk\n    Found existing installation: opentelemetry-sdk 1.35.0\n    Uninstalling opentelemetry-sdk-1.35.0:\n      Successfully uninstalled opentelemetry-sdk-1.35.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nopentelemetry-exporter-prometheus 0.56b0 requires opentelemetry-sdk~=1.35.0, but you have opentelemetry-sdk 1.37.0 which is incompatible.\nmlflow-skinny 2.21.3 requires packaging<25, but you have packaging 25.0 which is incompatible.\njupyterlab-nvdashboard 0.13.0 requires jupyterlab>=4, but you have jupyterlab 3.6.8 which is incompatible.\njupyter-resource-usage 0.7.2 requires psutil~=5.6, but you have psutil 7.1.0 which is incompatible.\ndask-sql 2024.5.0 requires dask[dataframe]>=2024.4.1, but you have dask 2023.2.0 which is incompatible.\ndask-sql 2024.5.0 requires distributed>=2024.4.1, but you have distributed 2023.2.0 which is incompatible.\nazureml-training-tabular 1.60.0 requires psutil<5.9.4,>=5.2.2, but you have psutil 7.1.0 which is incompatible.\nazureml-training-tabular 1.60.0 requires scipy<1.11.0,>=1.0.0, but you have scipy 1.11.0 which is incompatible.\nazureml-training-tabular 1.60.0 requires urllib3<2.0.0, but you have urllib3 2.5.0 which is incompatible.\nazureml-mlflow 1.60.0 requires azure-storage-blob<=12.19.0,>=12.5.0, but you have azure-storage-blob 12.26.0 which is incompatible.\nazureml-automl-runtime 1.60.0 requires psutil<5.9.4,>=5.2.2, but you have psutil 7.1.0 which is incompatible.\nazureml-automl-runtime 1.60.0 requires urllib3<2.0.0, but you have urllib3 2.5.0 which is incompatible.\nazureml-automl-dnn-nlp 1.60.0 requires torch==2.2.2, but you have torch 2.7.1 which is incompatible.\nadlfs 2024.12.0 requires fsspec>=2023.12.0, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed asgiref-3.10.0 azure-ai-ml-1.29.0 azure-core-tracing-opentelemetry-1.0.0b12 azure-monitor-opentelemetry-1.8.1 azure-monitor-opentelemetry-exporter-1.0.0b42 azure-storage-blob-12.26.0 azure-storage-file-datalake-12.21.0 azure-storage-file-share-12.22.0 fixedint-0.1.6 marshmallow-3.26.1 opentelemetry-api-1.37.0 opentelemetry-instrumentation-0.58b0 opentelemetry-instrumentation-asgi-0.58b0 opentelemetry-instrumentation-dbapi-0.58b0 opentelemetry-instrumentation-django-0.58b0 opentelemetry-instrumentation-fastapi-0.58b0 opentelemetry-instrumentation-flask-0.58b0 opentelemetry-instrumentation-psycopg2-0.58b0 opentelemetry-instrumentation-requests-0.58b0 opentelemetry-instrumentation-urllib-0.58b0 opentelemetry-instrumentation-urllib3-0.58b0 opentelemetry-instrumentation-wsgi-0.58b0 opentelemetry-resource-detector-azure-0.1.5 opentelemetry-sdk-1.37.0 opentelemetry-semantic-conventions-0.58b0 opentelemetry-util-http-0.58b0 psutil-7.1.0 pydash-8.0.5 strictyaml-1.7.3\n"
        }
      ],
      "execution_count": 1,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle to the workspace\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "# Authentication package\n",
        "from azure.identity import DefaultAzureCredential\n",
        "credential = DefaultAzureCredential()"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1759745079005
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a handle to the workspace\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=\"f2f9c878-1ca2-40dc-a868-b68e6b45074d\",  # Replace with your Azure subscription ID\n",
        "    resource_group_name=\"default_resource_group\",  # Replace with your resource group name\n",
        "    workspace_name=\"cars\",  # Replace with your ML workspace name\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1759745084039
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.2 Set Up Compute Cluster**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import AmlCompute\n",
        "\n",
        "# Name assigned to the compute cluster\n",
        "cpu_compute_target = \"cars\"\n",
        "\n",
        "try:\n",
        "    # let's see if the compute target already exists\n",
        "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
        "    print(\n",
        "        f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\"\n",
        "    )\n",
        "\n",
        "except Exception:\n",
        "    print(\"Creating a new cpu compute target...\")\n",
        "\n",
        "    # Let's create the Azure ML compute object with the intended parameters\n",
        "    cpu_cluster = AmlCompute(\n",
        "        name=cpu_compute_target,\n",
        "        # Azure ML Compute is the on-demand VM service\n",
        "        type=\"amlcompute\",\n",
        "        # VM Family\n",
        "        size=\"Standard_DS11_v2\",\n",
        "        # Minimum running nodes when there is no job running\n",
        "        min_instances=0,\n",
        "        # Nodes in cluster\n",
        "        max_instances=1,\n",
        "        # How many seconds will the node running after the job termination\n",
        "        idle_time_before_scale_down=180,\n",
        "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
        "        tier=\"Dedicated\",\n",
        "    )\n",
        "\n",
        "    # Now, we pass the object to MLClient's create_or_update method\n",
        "    cpu_cluster = ml_client.compute.begin_create_or_update(cpu_cluster).result()\n",
        "\n",
        "print(\n",
        "    f\"AMLCompute with name {cpu_cluster.name} is created, the compute size is {cpu_cluster.size}\"\n",
        ")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "You already have a cluster named cars, we'll reuse it as is.\nAMLCompute with name cars is created, the compute size is Standard_DS12_v2\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1759745088747
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.3 Register Dataset as Data Asset**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Data\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "# Path to the local dataset\n",
        "local_data_path = 'used_cars.csv'  # Updated filename to match workspace\n",
        "\n",
        "# Create and register the dataset as an AzureML data asset\n",
        "data_asset = Data(\n",
        "    path=local_data_path,\n",
        "    type=AssetTypes.URI_FILE, \n",
        "    description=\"A dataset of used cars for price prediction\",\n",
        "    name=\"used-cars-data\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1759745093211
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ml_client.data.create_or_update(data_asset)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "Data({'path': 'azureml://subscriptions/f2f9c878-1ca2-40dc-a868-b68e6b45074d/resourcegroups/default_resource_group/workspaces/cars/datastores/workspaceblobstore/paths/LocalUpload/0b8e06a9f14bf45a52b1c21394f1cdf03017517cd48663b3e20a05882ff35cdd/used_cars.csv', 'skip_validation': False, 'mltable_schema_url': None, 'referenced_uris': None, 'type': 'uri_file', 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'used-cars-data', 'description': 'A dataset of used cars for price prediction', 'tags': {}, 'properties': {}, 'print_as_yaml': False, 'id': '/subscriptions/f2f9c878-1ca2-40dc-a868-b68e6b45074d/resourceGroups/default_resource_group/providers/Microsoft.MachineLearningServices/workspaces/cars/data/used-cars-data/versions/2', 'Resource__source_path': '', 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/cars/code/Users/Rakshit_1746373710099', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7c0e1443dbd0>, 'serialize': <msrest.serialization.Serializer object at 0x7c0e1443d7b0>, 'version': '2', 'latest_version': None, 'datastore': None})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1759745098184
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1.4 Create and Configure Job Environment**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory for the preprocessing script\n",
        "import os\n",
        "\n",
        "src_dir_env = \"./env\"\n",
        "os.makedirs(src_dir_env, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1759745685744
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {src_dir_env}/conda.yml\n",
        "name: sklearn-env\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.8\n",
        "  - pip=21.2.4\n",
        "  - scikit-learn=0.23.2\n",
        "  - scipy=1.7.1\n",
        "  - pip:  \n",
        "    - mlflow==2.8.1\n",
        "    - azureml-mlflow==1.51.0\n",
        "    - azureml-inference-server-http\n",
        "    - azureml-core==1.49.0\n",
        "    - cloudpickle==1.6.0"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./env/conda.yml\n"
        }
      ],
      "execution_count": 27,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Environment, BuildContext\n",
        "\n",
        "env_docker_conda = Environment(\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n",
        "    conda_file=\"env/conda.yml\",\n",
        "    name=\"machine_learning_E2E\",\n",
        "    description=\"Environment created from a Docker image plus Conda environment.\",\n",
        ")\n",
        "ml_client.environments.create_or_update(env_docker_conda)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "Environment({'arm_type': 'environment_version', 'latest_version': None, 'image': 'mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04', 'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'machine_learning_E2E', 'description': 'Environment created from a Docker image plus Conda environment.', 'tags': {}, 'properties': {'azureml.labels': 'latest'}, 'print_as_yaml': False, 'id': '/subscriptions/f2f9c878-1ca2-40dc-a868-b68e6b45074d/resourceGroups/default_resource_group/providers/Microsoft.MachineLearningServices/workspaces/cars/environments/machine_learning_E2E/versions/1', 'Resource__source_path': '', 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/cars/code/Users/Rakshit_1746373710099', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7c0df28f3550>, 'serialize': <msrest.serialization.Serializer object at 0x7c0e2c10b970>, 'version': '1', 'conda_file': {'channels': ['conda-forge'], 'dependencies': ['python=3.8', 'pip=21.2.4', 'scikit-learn=0.23.2', 'scipy=1.7.1', {'pip': ['mlflow==2.8.1', 'azureml-mlflow==1.51.0', 'azureml-inference-server-http', 'azureml-core==1.49.0', 'cloudpickle==1.6.0']}], 'name': 'sklearn-env'}, 'build': None, 'inference_config': None, 'os_type': 'Linux', 'conda_file_path': None, 'path': None, 'datastore': None, 'upload_hash': None, 'translated_conda_file': '{\\n  \"channels\": [\\n    \"conda-forge\"\\n  ],\\n  \"dependencies\": [\\n    \"python=3.8\",\\n    \"pip=21.2.4\",\\n    \"scikit-learn=0.23.2\",\\n    \"scipy=1.7.1\",\\n    {\\n      \"pip\": [\\n        \"mlflow==2.8.1\",\\n        \"azureml-mlflow==1.51.0\",\\n        \"azureml-inference-server-http\",\\n        \"azureml-core==1.49.0\",\\n        \"cloudpickle==1.6.0\"\\n      ]\\n    }\\n  ],\\n  \"name\": \"sklearn-env\"\\n}'})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1759745107140
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Model Development Workflow**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.1 Data Preparation**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This **Data Preparation job** is designed to process an input dataset by splitting it into two parts: one for training the model and the other for testing it. The script accepts three inputs: the location of the input data (`used_cars.csv`), the ratio for splitting the data into training and testing sets (`test_train_ratio`), and the paths to save the resulting training (`train_data`) and testing (`test_data`) data. The script first reads the input CSV data from a data asset URI, then splits it using Scikit-learn's train_test_split function, and saves the two parts to the specified directories. It also logs the number of records in both the training and testing datasets using MLflow."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory for the data preparation script\n",
        "import os\n",
        "\n",
        "src_dir = \"./src\"\n",
        "os.makedirs(src_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1759745702652
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {src_dir}/data_prep.py\n",
        "\"\"\"\n",
        "Data Preparation Script for Used Cars Price Prediction\n",
        "This script handles data loading, cleaning, and splitting for the MLOps pipeline.\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import mlflow\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to execute data preparation\"\"\"\n",
        "    \n",
        "    # Input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
        "    parser.add_argument(\"--test_train_ratio\", type=float, required=False, default=0.25)\n",
        "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
        "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    # Start Logging\n",
        "    mlflow.start_run()\n",
        "    \n",
        "    print(\"Input Data:\", args.data)\n",
        "    print(\"Test/Train Ratio:\", args.test_train_ratio)\n",
        "    \n",
        "    # Read the data\n",
        "    print(\"Reading data...\")\n",
        "    all_data = pd.read_csv(args.data)\n",
        "    \n",
        "    print(f\"Dataset shape: {all_data.shape}\")\n",
        "    print(\"Dataset info:\")\n",
        "    print(all_data.info())\n",
        "    print(\"\\nDataset description:\")\n",
        "    print(all_data.describe())\n",
        "    \n",
        "    # Check for missing values\n",
        "    print(\"\\nMissing values:\")\n",
        "    print(all_data.isnull().sum())\n",
        "    \n",
        "    # Data preprocessing\n",
        "    print(\"\\nStarting data preprocessing...\")\n",
        "    \n",
        "    # Handle missing values if any\n",
        "    if all_data.isnull().sum().sum() > 0:\n",
        "        print(\"Handling missing values...\")\n",
        "        # For numeric columns, fill with median\n",
        "        numeric_cols = all_data.select_dtypes(include=['float64', 'int64']).columns\n",
        "        for col in numeric_cols:\n",
        "            if all_data[col].isnull().sum() > 0:\n",
        "                all_data[col].fillna(all_data[col].median(), inplace=True)\n",
        "        \n",
        "        # For categorical columns, fill with mode\n",
        "        categorical_cols = all_data.select_dtypes(include=['object']).columns\n",
        "        for col in categorical_cols:\n",
        "            if all_data[col].isnull().sum() > 0:\n",
        "                all_data[col].fillna(all_data[col].mode()[0], inplace=True)\n",
        "    \n",
        "    # Remove any duplicates\n",
        "    initial_rows = len(all_data)\n",
        "    all_data = all_data.drop_duplicates()\n",
        "    print(f\"Removed {initial_rows - len(all_data)} duplicate rows\")\n",
        "    \n",
        "    # Convert column names to lowercase for consistency\n",
        "    all_data.columns = all_data.columns.str.lower()\n",
        "    \n",
        "    # Split the data into train and test sets\n",
        "    print(\"Splitting data into train and test sets...\")\n",
        "    train_df, test_df = train_test_split(\n",
        "        all_data,\n",
        "        test_size=args.test_train_ratio,\n",
        "        random_state=42,\n",
        "        stratify=all_data['segment']  # Stratify by segment to maintain distribution\n",
        "    )\n",
        "    \n",
        "    # Create the output directories\n",
        "    os.makedirs(args.train_data, exist_ok=True)\n",
        "    os.makedirs(args.test_data, exist_ok=True)\n",
        "    \n",
        "    # Save the train and test sets\n",
        "    train_df.to_csv(os.path.join(args.train_data, \"train.csv\"), index=False)\n",
        "    test_df.to_csv(os.path.join(args.test_data, \"test.csv\"), index=False)\n",
        "    \n",
        "    # Log key metrics\n",
        "    mlflow.log_metric(\"total_samples\", len(all_data))\n",
        "    mlflow.log_metric(\"train_samples\", len(train_df))\n",
        "    mlflow.log_metric(\"test_samples\", len(test_df))\n",
        "    mlflow.log_metric(\"train_test_ratio\", args.test_train_ratio)\n",
        "    \n",
        "    print(f\"Total samples: {len(all_data)}\")\n",
        "    print(f\"Training samples: {len(train_df)}\")\n",
        "    print(f\"Testing samples: {len(test_df)}\")\n",
        "    print(f\"Test ratio: {args.test_train_ratio}\")\n",
        "    \n",
        "    # Log data distribution insights\n",
        "    print(\"\\nSegment distribution in training data:\")\n",
        "    segment_dist = train_df['segment'].value_counts()\n",
        "    print(segment_dist)\n",
        "    \n",
        "    for segment, count in segment_dist.items():\n",
        "        mlflow.log_metric(f\"train_{segment.replace(' ', '_')}_count\", count)\n",
        "    \n",
        "    print(\"\\nData preparation completed successfully!\")\n",
        "    \n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./src/data_prep.py\n"
        }
      ],
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1758472913127
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Define Data Preparation job**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this AzureML job, we define the `command` object that takes input files and output directories, then executes the script with the provided inputs and outputs. The job runs in a pre-configured AzureML environment with the necessary libraries. The result will be two separate datasets for training and testing, ready for use in subsequent steps of the machine learning pipeline."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import command, Input, Output\n",
        "\n",
        "# Get the data asset\n",
        "data_asset = ml_client.data.get(name=\"used-cars-data\", version=\"1\")\n",
        "\n",
        "# Define the data preparation job\n",
        "data_prep_job = command(\n",
        "    inputs=dict(\n",
        "        data=Input(type=\"uri_file\", path=data_asset.path),\n",
        "        test_train_ratio=0.2,\n",
        "    ),\n",
        "    outputs=dict(\n",
        "        train_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
        "        test_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
        "    ),\n",
        "    code=\"./src\",  # location of source code\n",
        "    command=\"python data_prep.py --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} --train_data ${{outputs.train_data}} --test_data ${{outputs.test_data}}\",\n",
        "    environment=\"machine_learning_E2E@latest\",\n",
        "    compute=\"cars\",\n",
        "    display_name=\"data_preparation\",\n",
        "    description=\"Data preparation for used cars price prediction\",\n",
        ")\n",
        "\n",
        "print(\"Data preparation job defined successfully!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Data preparation job defined successfully!\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1759745126895
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.2 Training the Model**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This Model Training job is designed to train a **Random Forest Regressor** on the dataset that was split into training and testing sets in the previous data preparation job. This job script accepts five inputs: the path to the training data (`train_data`), the path to the testing data (`test_data`), the number of trees in the forest (`n_estimators`, with a default value of 100), the maximum depth of the trees (`max_depth`, which is set to None by default), and the path to save the trained model (`model_output`).\n",
        "\n",
        "The script begins by reading the training and testing data files, then processes the data to separate features (X) and target labels (y). A Random Forest Regressor model is initialized using the given n_estimators and max_depth, and it is trained using the training data. The model's performance is evaluated using the `Mean Squared Error (MSE)`. The MSE score is logged in MLflow. Finally, the trained model is saved and stored in the specified output location as an MLflow model. The job completes by logging the final MSE score and ending the MLflow run.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {src_dir}/train.py\n",
        "\"\"\"\n",
        "Model Training Script for Used Cars Price Prediction\n",
        "This script handles model training with hyperparameter tuning and evaluation.\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import joblib\n",
        "\n",
        "def preprocess_features(df, label_encoders=None, scaler=None, is_training=True):\n",
        "    \"\"\"Preprocess features for training or prediction\"\"\"\n",
        "    \n",
        "    # Separate features and target\n",
        "    if 'price' in df.columns:\n",
        "        X = df.drop(['price'], axis=1)\n",
        "        y = df['price']\n",
        "    else:\n",
        "        X = df.copy()\n",
        "        y = None\n",
        "    \n",
        "    # Handle categorical variables\n",
        "    categorical_cols = ['segment']\n",
        "    \n",
        "    if is_training:\n",
        "        label_encoders = {}\n",
        "        for col in categorical_cols:\n",
        "            if col in X.columns:\n",
        "                le = LabelEncoder()\n",
        "                X[col] = le.fit_transform(X[col].astype(str))\n",
        "                label_encoders[col] = le\n",
        "        \n",
        "        # Scale numerical features\n",
        "        numerical_cols = ['kilometers_driven', 'mileage', 'engine', 'power', 'seats']\n",
        "        scaler = StandardScaler()\n",
        "        X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
        "        \n",
        "    else:\n",
        "        # Apply existing encoders and scaler\n",
        "        for col in categorical_cols:\n",
        "            if col in X.columns and col in label_encoders:\n",
        "                X[col] = label_encoders[col].transform(X[col].astype(str))\n",
        "        \n",
        "        numerical_cols = ['kilometers_driven', 'mileage', 'engine', 'power', 'seats']\n",
        "        X[numerical_cols] = scaler.transform(X[numerical_cols])\n",
        "    \n",
        "    return X, y, label_encoders, scaler\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to execute model training\"\"\"\n",
        "    \n",
        "    # Input and output arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
        "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
        "    parser.add_argument(\"--n_estimators\", type=int, required=False, default=100)\n",
        "    parser.add_argument(\"--max_depth\", type=int, required=False, default=None)\n",
        "    parser.add_argument(\"--model_output\", type=str, help=\"path to model file\")\n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    # Start Logging\n",
        "    mlflow.start_run()\n",
        "    \n",
        "    # Log parameters\n",
        "    mlflow.log_param(\"n_estimators\", args.n_estimators)\n",
        "    mlflow.log_param(\"max_depth\", args.max_depth)\n",
        "    \n",
        "    print(\"Loading training data...\")\n",
        "    train_df = pd.read_csv(os.path.join(args.train_data, \"train.csv\"))\n",
        "    \n",
        "    print(\"Loading testing data...\")\n",
        "    test_df = pd.read_csv(os.path.join(args.test_data, \"test.csv\"))\n",
        "    \n",
        "    print(f\"Training data shape: {train_df.shape}\")\n",
        "    print(f\"Testing data shape: {test_df.shape}\")\n",
        "    \n",
        "    # Preprocess the data\n",
        "    print(\"Preprocessing training data...\")\n",
        "    X_train, y_train, label_encoders, scaler = preprocess_features(train_df, is_training=True)\n",
        "    \n",
        "    print(\"Preprocessing testing data...\")\n",
        "    X_test, y_test, _, _ = preprocess_features(test_df, label_encoders, scaler, is_training=False)\n",
        "    \n",
        "    print(f\"Training features shape: {X_train.shape}\")\n",
        "    print(f\"Testing features shape: {X_test.shape}\")\n",
        "    \n",
        "    # Initialize and train the model\n",
        "    print(\"Training Random Forest model...\")\n",
        "    max_depth = args.max_depth if args.max_depth != -1 else None\n",
        "    \n",
        "    model = RandomForestRegressor(\n",
        "        n_estimators=args.n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    print(\"Making predictions...\")\n",
        "    train_predictions = model.predict(X_train)\n",
        "    test_predictions = model.predict(X_test)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    train_mse = mean_squared_error(y_train, train_predictions)\n",
        "    test_mse = mean_squared_error(y_test, test_predictions)\n",
        "    train_r2 = r2_score(y_train, train_predictions)\n",
        "    test_r2 = r2_score(y_test, test_predictions)\n",
        "    train_mae = mean_absolute_error(y_train, train_predictions)\n",
        "    test_mae = mean_absolute_error(y_test, test_predictions)\n",
        "    \n",
        "    # Log metrics\n",
        "    mlflow.log_metric(\"train_mse\", train_mse)\n",
        "    mlflow.log_metric(\"test_mse\", test_mse)\n",
        "    mlflow.log_metric(\"train_r2\", train_r2)\n",
        "    mlflow.log_metric(\"test_r2\", test_r2)\n",
        "    mlflow.log_metric(\"train_mae\", train_mae)\n",
        "    mlflow.log_metric(\"test_mae\", test_mae)\n",
        "    \n",
        "    print(f\"Training MSE: {train_mse:.4f}\")\n",
        "    print(f\"Testing MSE: {test_mse:.4f}\")\n",
        "    print(f\"Training R²: {train_r2:.4f}\")\n",
        "    print(f\"Testing R²: {test_r2:.4f}\")\n",
        "    print(f\"Training MAE: {train_mae:.4f}\")\n",
        "    print(f\"Testing MAE: {test_mae:.4f}\")\n",
        "    \n",
        "    # Create the output directory\n",
        "    os.makedirs(args.model_output, exist_ok=True)\n",
        "    \n",
        "    # Save the model and preprocessors\n",
        "    model_path = os.path.join(args.model_output, \"model.pkl\")\n",
        "    joblib.dump(model, model_path)\n",
        "    \n",
        "    preprocessors_path = os.path.join(args.model_output, \"preprocessors.pkl\")\n",
        "    preprocessors = {\n",
        "        'label_encoders': label_encoders,\n",
        "        'scaler': scaler\n",
        "    }\n",
        "    joblib.dump(preprocessors, preprocessors_path)\n",
        "    \n",
        "    # Log the model\n",
        "    mlflow.sklearn.log_model(\n",
        "        sk_model=model,\n",
        "        artifact_path=\"model\",\n",
        "        signature=None\n",
        "    )\n",
        "    \n",
        "    print(\"Model training completed successfully!\")\n",
        "    \n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./src/train.py\n"
        }
      ],
      "execution_count": 31,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Define Model Training Job**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this AzureML job, we define the `command` object that takes the paths to the training and testing data, the number of trees in the forest (`n_estimators`), and the maximum depth of the trees (`max_depth`) as inputs, and outputs the trained model. The command runs in a pre-configured AzureML environment with all the necessary libraries. The job produces a trained **Random Forest Regressor model**, which can be used for predicting the price of used cars based on the given attributes."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training job\n",
        "train_job = command(\n",
        "    inputs=dict(\n",
        "        train_data=Input(type=\"uri_folder\"),\n",
        "        test_data=Input(type=\"uri_folder\"),\n",
        "        n_estimators=100,\n",
        "        max_depth=-1,  # -1 represents None\n",
        "    ),\n",
        "    outputs=dict(\n",
        "        model_output=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
        "    ),\n",
        "    code=\"./src\",  # location of source code\n",
        "    command=\"python train.py --train_data ${{inputs.train_data}} --test_data ${{inputs.test_data}} --n_estimators ${{inputs.n_estimators}} --max_depth ${{inputs.max_depth}} --model_output ${{outputs.model_output}}\",\n",
        "    environment=\"machine_learning_E2E@latest\",\n",
        "    compute=\"cars\",\n",
        "    display_name=\"train_model\",\n",
        "    description=\"Train Random Forest model for used cars price prediction\",\n",
        ")\n",
        "\n",
        "print(\"Training job defined successfully!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Training job defined successfully!\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1759745135441
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3 Registering the Best Trained Model**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Model Registration job** is designed to take the best-trained model from the hyperparameter tuning sweep job and register it in MLflow as a versioned artifact for future use in the used car price prediction pipeline. This job script accepts one input: the path to the trained model (model). The script begins by loading the model using the `mlflow.sklearn.load_model()` function. Afterward, it registers the model in the MLflow model registry, assigning it a descriptive name (`used_cars_price_prediction_model`) and specifying an artifact path (`random_forest_price_regressor`) where the model artifacts will be stored. Using MLflow's `log_model()` function, the model is logged along with its metadata, ensuring that the model is easily trackable and retrievable for future evaluation, deployment, or retraining."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {src_dir}/model_register.py\n",
        "\"\"\"\n",
        "Model Registration Script for Used Cars Price Prediction\n",
        "This script registers the best trained model in MLflow registry.\n",
        "\"\"\"\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import joblib\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to execute model registration\"\"\"\n",
        "    \n",
        "    # Input arguments\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--model\", type=str, help=\"path to model file\")\n",
        "    args = parser.parse_args()\n",
        "    \n",
        "    # Start Logging\n",
        "    mlflow.start_run()\n",
        "    \n",
        "    print(f\"Loading model from: {args.model}\")\n",
        "    \n",
        "    # Load the model\n",
        "    model_path = os.path.join(args.model, \"model.pkl\")\n",
        "    model = joblib.load(model_path)\n",
        "    \n",
        "    # Load preprocessors\n",
        "    preprocessors_path = os.path.join(args.model, \"preprocessors.pkl\")\n",
        "    preprocessors = joblib.load(preprocessors_path)\n",
        "    \n",
        "    print(\"Model loaded successfully!\")\n",
        "    print(f\"Model type: {type(model)}\")\n",
        "    \n",
        "    # Register the model in MLflow\n",
        "    print(\"Registering model in MLflow...\")\n",
        "    \n",
        "    # Log the model with all artifacts\n",
        "    mlflow.sklearn.log_model(\n",
        "        sk_model=model,\n",
        "        artifact_path=\"random_forest_price_regressor\",\n",
        "        registered_model_name=\"used_cars_price_prediction_model\",\n",
        "        signature=None,  # You can add model signature here for better tracking\n",
        "        input_example=None  # You can add input example here\n",
        "    )\n",
        "    \n",
        "    # Log preprocessors as artifacts\n",
        "    mlflow.log_artifact(preprocessors_path, \"preprocessors\")\n",
        "    \n",
        "    print(\"Model registered successfully in MLflow!\")\n",
        "    print(\"Model name: used_cars_price_prediction_model\")\n",
        "    print(\"Artifact path: random_forest_price_regressor\")\n",
        "    \n",
        "    # Stop Logging\n",
        "    mlflow.end_run()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Writing ./src/model_register.py\n"
        }
      ],
      "execution_count": 32,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Define Model Register Job**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this AzureML job, a `command` object is defined to execute the `model_register.py` script. It accepts the best-trained model as input, runs the script in the `AzureML-sklearn-1.0-ubuntu20.04-py38-cpu` environment, and uses the same compute cluster as the previous jobs (`cars`). This job plays a crucial role in the pipeline by ensuring that the best-performing model identified during hyperparameter tuning is systematically stored and made available in the MLflow registry for further evaluation, deployment, or retraining. Integrating this job into the end-to-end pipeline automates the process of registering high-quality models, completing the model development lifecycle and enabling the prediction of used car prices."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model registration job\n",
        "register_job = command(\n",
        "    inputs=dict(\n",
        "        model=Input(type=\"uri_folder\"),\n",
        "    ),\n",
        "    code=\"./src\",  # location of source code\n",
        "    command=\"python model_register.py --model ${{inputs.model}}\",\n",
        "    environment=\"machine_learning_E2E@latest\",\n",
        "    compute=\"cars\",\n",
        "    display_name=\"register_model\",\n",
        "    description=\"Register the trained model in MLflow registry\",\n",
        ")\n",
        "\n",
        "print(\"Model registration job defined successfully!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Model registration job defined successfully!\n"
        }
      ],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1759745144778
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.4. Assembling the End-to-End Workflow**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The end-to-end pipeline integrates all the previously defined jobs into a seamless workflow, automating the process of data preparation, model training, hyperparameter tuning, and model registration. The pipeline is designed using Azure Machine Learning's `@pipeline` decorator, specifying the compute target and providing a detailed description of the workflow."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import dsl\n",
        "\n",
        "@dsl.pipeline(\n",
        "    compute=\"cars\",\n",
        "    description=\"End-to-End MLOps Pipeline for Used Cars Price Prediction\",\n",
        ")\n",
        "def used_cars_pipeline(\n",
        "    pipeline_data,\n",
        "    test_train_ratio=0.2,\n",
        "    n_estimators=100,\n",
        "    max_depth=-1,\n",
        "):\n",
        "    \"\"\"\n",
        "    End-to-end pipeline for used cars price prediction\n",
        "    \n",
        "    Args:\n",
        "        pipeline_data: Input dataset\n",
        "        test_train_ratio: Ratio for train-test split\n",
        "        n_estimators: Number of trees in Random Forest\n",
        "        max_depth: Maximum depth of trees\n",
        "    \"\"\"\n",
        "    \n",
        "    # Step 1: Data Preparation\n",
        "    data_prep_step = data_prep_job(\n",
        "        data=pipeline_data,\n",
        "        test_train_ratio=test_train_ratio,\n",
        "    )\n",
        "    \n",
        "    # Step 2: Model Training\n",
        "    train_step = train_job(\n",
        "        train_data=data_prep_step.outputs.train_data,\n",
        "        test_data=data_prep_step.outputs.test_data,\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "    )\n",
        "    \n",
        "    # Step 3: Model Registration\n",
        "    register_step = register_job(\n",
        "        model=train_step.outputs.model_output,\n",
        "    )\n",
        "    \n",
        "    # Return outputs\n",
        "    return {\n",
        "        \"train_data\": data_prep_step.outputs.train_data,\n",
        "        \"test_data\": data_prep_step.outputs.test_data,\n",
        "        \"model\": train_step.outputs.model_output,\n",
        "    }\n",
        "\n",
        "# Create pipeline instance\n",
        "pipeline = used_cars_pipeline(\n",
        "    pipeline_data=Input(type=\"uri_file\", path=data_asset.path),\n",
        "    test_train_ratio=0.2,\n",
        "    n_estimators=100,\n",
        "    max_depth=-1,\n",
        ")\n",
        "\n",
        "print(\"Pipeline defined successfully!\")\n",
        "print(\"Pipeline components:\")\n",
        "print(\"1. Data Preparation - Splits data into train/test sets\")\n",
        "print(\"2. Model Training - Trains Random Forest model\")\n",
        "print(\"3. Model Registration - Registers model in MLflow\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Pipeline defined successfully!\nPipeline components:\n1. Data Preparation - Splits data into train/test sets\n2. Model Training - Trains Random Forest model\n3. Model Registration - Registers model in MLflow\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1759745153321
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Pipeline Execution and Monitoring**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3.1 Submit and Run the Pipeline**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit the pipeline job\n",
        "pipeline_job = ml_client.jobs.create_or_update(\n",
        "    pipeline,\n",
        "    experiment_name=\"used-cars-price-prediction\"\n",
        ")\n",
        "\n",
        "print(f\"Pipeline job submitted with ID: {pipeline_job.name}\")\n",
        "print(f\"Pipeline status: {pipeline_job.status}\")\n",
        "print(f\"Pipeline URL: {pipeline_job.services['Studio'].endpoint}\")\n",
        "\n",
        "# You can uncomment the following line to wait for the pipeline to complete\n",
        "# ml_client.jobs.stream(pipeline_job.name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Class AutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass AutoDeleteConditionSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseAutoDeleteSettingSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass IntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass ProtectionLevelSchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\nClass BaseIntellectualPropertySchema: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\npathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\npathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\npathOnCompute is not a known attribute of class <class 'azure.ai.ml._restclient.v2023_04_01_preview.models._models_py3.UriFolderJobOutput'> and will be ignored\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Pipeline job submitted with ID: calm_bean_8qrbm1d85q\nPipeline status: NotStarted\nPipeline URL: https://ml.azure.com/runs/calm_bean_8qrbm1d85q?wsid=/subscriptions/f2f9c878-1ca2-40dc-a868-b68e6b45074d/resourcegroups/default_resource_group/workspaces/cars&tid=a2799098-ec71-4199-a883-6274017f5282\n"
        }
      ],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1759745171161
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. CI/CD with GitHub Actions**\n",
        "\n",
        "### **4.1 Setting up GitHub Repository**\n",
        "\n",
        "This section demonstrates how to set up a GitHub repository with CI/CD capabilities using GitHub Actions for our MLOps pipeline. The workflow will automatically trigger the Azure ML pipeline when code changes are pushed to the repository."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create GitHub Actions workflow\n",
        "import os\n",
        "\n",
        "# Create .github/workflows directory\n",
        "workflows_dir = \"./.github/workflows\"\n",
        "os.makedirs(workflows_dir, exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1759745177544
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {workflows_dir}/mlops-pipeline.yml\n",
        "name: MLOps Pipeline - Used Cars Price Prediction\n",
        "\n",
        "on:\n",
        "  push:\n",
        "    branches: [ main, develop ]\n",
        "  pull_request:\n",
        "    branches: [ main ]\n",
        "  workflow_dispatch:\n",
        "\n",
        "env:\n",
        "  AZURE_ML_WORKSPACE: your-ml-workspace\n",
        "  AZURE_RESOURCE_GROUP: your-resource-group\n",
        "  AZURE_SUBSCRIPTION_ID: your-subscription-id\n",
        "\n",
        "jobs:\n",
        "  data-validation:\n",
        "    runs-on: ubuntu-latest\n",
        "    steps:\n",
        "    - uses: actions/checkout@v3\n",
        "    \n",
        "    - name: Set up Python\n",
        "      uses: actions/setup-python@v4\n",
        "      with:\n",
        "        python-version: '3.8'\n",
        "    \n",
        "    - name: Install dependencies\n",
        "      run: |\n",
        "        python -m pip install --upgrade pip\n",
        "        pip install pandas scikit-learn pytest\n",
        "    \n",
        "    - name: Run data validation tests\n",
        "      run: |\n",
        "        python -m pytest tests/ -v\n",
        "\n",
        "  model-training:\n",
        "    needs: data-validation\n",
        "    runs-on: ubuntu-latest\n",
        "    steps:\n",
        "    - uses: actions/checkout@v3\n",
        "    \n",
        "    - name: Set up Python\n",
        "      uses: actions/setup-python@v4\n",
        "      with:\n",
        "        python-version: '3.8'\n",
        "    \n",
        "    - name: Install Azure CLI and ML extension\n",
        "      run: |\n",
        "        curl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\n",
        "        az extension add -n ml\n",
        "    \n",
        "    - name: Azure Login\n",
        "      uses: azure/login@v1\n",
        "      with:\n",
        "        creds: ${{ secrets.AZURE_CREDENTIALS }}\n",
        "    \n",
        "    - name: Install Python dependencies\n",
        "      run: |\n",
        "        python -m pip install --upgrade pip\n",
        "        pip install azure-ai-ml azure-identity\n",
        "\n",
        "    - name: Run MLOps Pipeline\n",
        "      run: |\n",
        "        python .github/workflows/run_pipeline.py\n",
        "      env:\n",
        "        AZURE_SUBSCRIPTION_ID: ${{ env.AZURE_SUBSCRIPTION_ID }}\n",
        "        AZURE_RESOURCE_GROUP: ${{ env.AZURE_RESOURCE_GROUP }}\n",
        "        AZURE_ML_WORKSPACE: ${{ env.AZURE_ML_WORKSPACE }}\n",
        "\n",
        "  model-deployment:\n",
        "    needs: model-training\n",
        "    runs-on: ubuntu-latest\n",
        "    if: github.ref == 'refs/heads/main'\n",
        "    steps:\n",
        "    - uses: actions/checkout@v3\n",
        "    \n",
        "    - name: Set up Python\n",
        "      uses: actions/setup-python@v4\n",
        "      with:\n",
        "        python-version: '3.8'\n",
        "    \n",
        "    - name: Azure Login\n",
        "      uses: azure/login@v1\n",
        "      with:\n",
        "        creds: ${{ secrets.AZURE_CREDENTIALS }}\n",
        "    \n",
        "    - name: Deploy Model\n",
        "      run: |\n",
        "        echo \"Model deployment step - implement based on your deployment strategy\"\n",
        "        # Add your model deployment logic here"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./.github/workflows/mlops-pipeline.yml\n"
        }
      ],
      "execution_count": 34,
      "metadata": {
        "gather": {
          "logged": 1758473046344
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile {workflows_dir}/run_pipeline.py\n",
        "\"\"\"\n",
        "Pipeline runner script for GitHub Actions\n",
        "This script submits the MLOps pipeline to Azure ML\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "from azure.ai.ml import MLClient, Input, dsl, command\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "def main():\n",
        "    # Get environment variables\n",
        "    subscription_id = os.environ[\"AZURE_SUBSCRIPTION_ID\"]\n",
        "    resource_group = os.environ[\"AZURE_RESOURCE_GROUP\"] \n",
        "    workspace_name = os.environ[\"AZURE_ML_WORKSPACE\"]\n",
        "    \n",
        "    # Initialize ML Client\n",
        "    credential = DefaultAzureCredential()\n",
        "    ml_client = MLClient(\n",
        "        credential=credential,\n",
        "        subscription_id=subscription_id,\n",
        "        resource_group_name=resource_group,\n",
        "        workspace_name=workspace_name,\n",
        "    )\n",
        "    \n",
        "    print(f\"Connected to workspace: {workspace_name}\")\n",
        "    \n",
        "    # Get the data asset\n",
        "    try:\n",
        "        data_asset = ml_client.data.get(name=\"used-cars-data\", version=\"1\")\n",
        "        print(\"Data asset found successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting data asset: {e}\")\n",
        "        return\n",
        "    \n",
        "    # Submit the pipeline (you would need to recreate the pipeline definition here\n",
        "    # or import it from a separate module)\n",
        "    print(\"Pipeline would be submitted here...\")\n",
        "    print(\"This is a placeholder for the actual pipeline submission logic\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting ./.github/workflows/run_pipeline.py\n"
        }
      ],
      "execution_count": 33,
      "metadata": {
        "gather": {
          "logged": 1759745220743
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Business Insights and Recommendations**\n",
        "\n",
        "### **5.1 Key Findings from the MLOps Pipeline**\n",
        "\n",
        "Based on the implementation of this end-to-end MLOps pipeline for used car price prediction, here are the key insights and recommendations for the Las Vegas automobile dealership:\n",
        "\n",
        "#### **Technical Insights:**\n",
        "\n",
        "1. **Automated Data Processing**: The pipeline automatically handles data cleaning, preprocessing, and feature engineering, ensuring consistent data quality across all pricing evaluations.\n",
        "\n",
        "2. **Model Performance**: The Random Forest Regressor provides robust predictions by considering multiple features like segment, mileage, engine capacity, power, and seating capacity.\n",
        "\n",
        "3. **Scalability**: The Azure ML infrastructure allows for easy scaling as the dealership grows and processes more data.\n",
        "\n",
        "#### **Business Benefits:**\n",
        "\n",
        "1. **Improved Pricing Accuracy**: \n",
        "   - Systematic data-driven approach reduces manual pricing errors\n",
        "   - Consistent evaluation criteria across all vehicles\n",
        "   - Real-time price updates based on market conditions\n",
        "\n",
        "2. **Operational Efficiency**:\n",
        "   - Automated pipeline reduces manual intervention\n",
        "   - Faster processing of new inventory\n",
        "   - Standardized workflows across the organization\n",
        "\n",
        "3. **Enhanced Customer Trust**:\n",
        "   - Transparent, data-driven pricing methodology\n",
        "   - Consistent pricing standards\n",
        "   - Reduced price discrepancies\n",
        "\n",
        "#### **Recommendations for Implementation:**\n",
        "\n",
        "1. **Data Quality Management**:\n",
        "   - Implement regular data validation checks\n",
        "   - Establish data governance policies\n",
        "   - Monitor for data drift and model performance degradation\n",
        "\n",
        "2. **Continuous Improvement**:\n",
        "   - Regular model retraining with new data\n",
        "   - A/B testing for different pricing strategies\n",
        "   - Integration of external market data (fuel prices, economic indicators)\n",
        "\n",
        "3. **Business Integration**:\n",
        "   - Train staff on the new automated system\n",
        "   - Establish feedback loops for model improvement\n",
        "   - Create dashboard for business stakeholders\n",
        "\n",
        "4. **Risk Management**:\n",
        "   - Implement model monitoring and alerting\n",
        "   - Establish fallback procedures for system failures\n",
        "   - Regular audits of pricing decisions\n",
        "\n",
        "### **5.2 Expected Business Impact**\n",
        "\n",
        "1. **Revenue Optimization**: More accurate pricing leads to better profit margins and competitive positioning\n",
        "2. **Customer Satisfaction**: Consistent and fair pricing improves customer trust and satisfaction\n",
        "3. **Operational Costs**: Reduced manual effort in pricing evaluations\n",
        "4. **Market Responsiveness**: Ability to quickly adapt to market changes\n",
        "5. **Scalability**: Infrastructure supports business growth without proportional increase in operational complexity\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. GitHub Repository**\n",
        "\n",
        "### **GitHub Repository Link**\n",
        "\n",
        "🔗 **GitHub Repository**: [Add your public GitHub repository link here]\n",
        "\n",
        "The repository contains:\n",
        "- Complete source code for the MLOps pipeline\n",
        "- Data preparation, training, and registration scripts\n",
        "- GitHub Actions workflow for CI/CD\n",
        "- Test files for data validation\n",
        "- Documentation and README\n",
        "\n",
        "Make sure the repository is public and contains all the necessary files for the MLOps pipeline implementation."
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "display_name": "Python 3.10 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}